Problem Statement:

* Create an ELT pipeline that ingests a CSV dataset (choose any sufficiently dense source eg. https://www.kaggle.com/datasets/abdullah0a/telecom-customer-churn-insights-for-analysis).

* Load up the dataset into a staging database of your choice.

* Design a transformation layer to process the input dataset for missing values (use defaults) and anonymising PII.

* The destination for the processed data should be a database ideal for generating reports.

* Establish an orchestration workflow for this pipeline to accept a feed every hour (should be configurable).

* Integrate any open-source reporting tool to generate statistics about the flow.

* Ensure the entire setup is available through composable container definition(s).

Tech Stack:

* Language/frameworks/solutions of your choice. Please just ensure, the solution is easy to run on a laptop.

* Please use open-source solutions wherever possible.

Delivery:

* Please share the entire source code as a public Github repository.

* Do add relevant instructions to run the code.

* Please also ensure it stays accessible for the duration of the discussions with HGI.

ETA:

* Please ensure the assignment is completed in about 24 hours (can be split over days if practical schedules demand).

* Also Please note-the solution selected should be scalable.